---
title: 'Web Scraping with RVest'
author: "Wouter van Atteveldt & Kasper Welbers"
date: "2020-10"
output: 
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r, echo=F, message=F}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, message = FALSE, warning = FALSE, fig.keep='none')
library(printr)
```

# Web scraping 

The internet is a veritable data gold mine, and being able to mine this data is a valuable skill set for all sorts of research. 
In this tutorial we will be looking at a technique called **web scraping**, which can greatly expand your horizon in terms of what data you will be able to collect. 

To put this into perspective, let's distinguish three general ways to gather online data. In the most straightforward situation, you can just **download** some data, for instance as a CSV or JSON file. 
This is great if it's possible, but it's often not an option. Another convenient situation is that some platforms have an **API**. For example, Twitter has an API where one can collect tweets for a given search term or user. 

But what if you encounter data that can't be downloaded, and for which no API is available? In this case, you might still be able to collect it using **web scraping**. A simple example might be a table on a website, as we'll show in a minute. A more elaborate example could be that you want to gather all user posts on a web forum, or all press releases from the website of a certain organization. 


## How does it work?

The vast majority of the internet users **HTML** to make nice looking web pages. 
Simply put, HTML is a markup language that tells a browser what things are shown where.
For example, it could say that halfway on the page there is a table, and then tell what data there is in the rows and columns. 

The key realization here is that HTML has a very simple structure. 


## A quick introduction to HTML

You don't need a deep understanding of HTML to do web scraping, but it's good to understand the main ideas.
To get a feel for HTML code, open [this link here]() in your web browser. If you have it, use Chrome or Firefox. When you view this HTML page in your browser, you see a nicely formatted document. Sure, it's not very pretty, but it does have a big bold title, and two ok-ish looking tables.

The purpose of this page is to show an easy example of what the HTML code looks like. If you right-click on the page, you should see an option like *view page source*. If you select it you'll see the entire HTML source code. Somewhere in the code you'll see: 

```{r, eval=F}
<table class="myTable" id="firstTable">
  <tr class="headerRow">
    <th>First column</th>
    <th>Second column</th>
    <th>Third column</th>
  </tr>
  <tr>
    <td>Some data</td>
    <td>Some other data</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>Daaata</td>
    <td>Da-ta</td>
    <td>Data?</td>
  </tr>
</table> 
```

This is the HTML representation of the table, and it's a good showcase of what HTML is about.
First of all, notice the tree shape.
At the highest level we have the *table*.
This table starts at the opening tag `<table>`, and ends at the closing tag `</table>` (notice the `/`).
Within this table we have 3 *table rows* `<tr>`.
Within the first row we have 3 *table headers* `<th>`, which contain the column names.
The second and third row each have 3 *table data* `<td>`, that contain the data points.

OK, let's slow down and just focus on the simple elegance at play here.
We see that all the data is within the `<table>...</table>` tags.
Let's call this entire unit the `<table>` **node**. 
To get this data, all we need to do is query (i.e.) look for this node. 

This is where the `rvest` package comes in.
With `rvest` we can read the HTML code of a web page into R, look for HTML nodes, and extract information from them.  
In our example, we see that the table has an id `firstTable`. 
Let's use this to look for it.

```{r}

```


All the data that we see inside of the table on the web page is between the `<table>...</table>` tags. 
To gather this data, we only need to know how to query (i.e. look for) 


To gather this data, the only thing that we need to know is how to *query* (i.e. look for) the table in the HTML code. 



It turns out that this is rather simple, because HTML nodes often have classes and/or IDs.
With the `rvest` package, we can load the HTML code of a web page into R, and then look for these 

The `rvest` package makes this really easy. 


```{r}

```





The key realization here is that HTML 


It takes only a little knowledge of HTML to understand how 


For this website, go to the web page that shows 



The thing to understand is that this 



By understanding a little bit of how this works, we can 

For example, 

For example, on [This wikipedia page](https://en.wikipedia.org/wiki/World_Happiness_Report) there are several tables. 


](this Wikipedia page)


For example, the following HTML code would tell a browser to show a headline with two paragraphs.
(note that you can't run this code in R)

```{r, eval=F}
<h2>Headline</h2>
<p>First paragraph</p>
<p>Second paragraph</p>
```





The goal of HTML is to make things look nice for humans. 



In your browser, this looks like a regular table, where the first row contains the column names ()

To see how this is possible, it's important to realize that the vast majority of the internet uses HTML to make good looking web pages. 
HTML is a markup language that 










But for the vast majority of online data, 

As we show in the motivation example below, this data can still quite easily be harvested we know a little bit about *Web scraping*.



But just as with an actual gold mine, you often can't just walk in and grab things. 




But

## Web scraping with the rvest package

`rvest` (think "harvest") is a package that allows you to retrieve and parse HTML pages in order to extract information from them.
How easy it is to do so ranges from the almost trivial for simple and well-designed web sites,
to almost impossible for web sites that actively try to prevent it.

As a general rule, you should make sure not to break any laws and respect copyright and terms of service
(although whether these terms are binding without you explicitly agreeing probably depends on your jurisdiction). 

Finally, web scraping is finicky and your scraper will probably stop functioning whenever the target web site is redesigned. 
For this reason, if there is an API you can use (e.g. for the guardian)
or there is another way to get the data without having to scrape it that will generally be preferable. 

## Motivational example

The example below scrapes the world happiness report from Wikipedia and shows the relationship between wealth and life expectancy.

```{r}
library(rvest)
library(tidyverse)

url = "https://en.wikipedia.org/wiki/World_Happiness_Report"

happy = url %>%
  read_html() %>% 
  html_node(".wikitable") %>% 
  html_table()

ggplot(happy, aes(x=`GDP per capita`, y=`Healthy life expectancy`)) + 
  geom_point() + geom_smooth(method = 'lm')
```



# Steps in web scraping

In general, the first step is to understand the structure of the web site you want to parse, and determine how you can find the information you are after. For this, the network view and inspect functions in most modern browsers are invaluable tools. 

After this, you can  retrieve the desired web page and parse the resulting HTML code. 
From this, you extract the required information using the patterns identified in the first step. 
In many cases, this information will contain or consist of links to other web pages,
in which case you will then retrieve and parse these pages as well. 

# Parsing HTML using CSS selectors

In theory, you could perform these steps using basic R commands to download files over the internet and parse the resulting HTML code as a text, e.g. using regular expressions. 
Parsing HTML correctly with pure regular expressions, however, is theoretically impossible and also a bad idea, because the same HTML structure can be expressed in multiple ways and HTML is very verbose.
Thus, it is a much better idea to parse the HTML and treat it as a tree structure. For example, consider the structure of a simple HTML fragment:

```{r}
html = "<body><div id='doc1'><p class='lead'>The lead</p><p>The body</p></div></body>"
d = xml2::read_html(html)


```

As you can see, both paragraphs (`p`) are nested in a `div`, which is nested in a `body`. 
The easiest way to identify an element in such a tree is using CSS selectors.
CSS (cascading style sheets) are designed to apply styling to certain aspects of a document, for example to format the lead paragraph in a different font. 
As part of such a style sheet, the selectors are used to identify which elements to apply styling to.

In scraping, we can use the same selectors to identify which tags contain the information we are interested in. Below is a table with some of the more common css selectors, 
but if you google something like 'css selector cheat sheet' you will find a lot of more exhaustive lists. 

| selector | example | meaning |
| --- | --- | --- |
| child | div > p | any `p` directly nested in a div |
| descendant | div p | any `p` below a div in the hierachy |
| first child | p:first-child | a `p` that is the first child of its parent |
| class | p.lead | a `p` with `class='lead'` |
| class | .lead | any tag with `class='lead'` |
| id | div#doc1 | a `div` with `id='doc1'` |

These selectors can be combined, so e.g. `body > #doc1 p.lead` identifies any `p` that has `class='lead'`, is a descendant of a tag with `id='doc1'` which is a direct child of `body`. 

Using rvest, you can easily extract information using such a selector:

```{r}
lead = html_node(d, "div .lead")
lead
html_text(lead)
```

As an alternative to CSS selectors, you can also use xpath. 
Xpath patterns are more difficult to read and write, 
but are in the end more powerful than CSS selectors, meaning there
are some patterns that you can express in xpath but cannot express in css.
For example, in CSS you cannot use arbitrary attributes to select a nore 
(such as `data="report"`), which is possible in xpath. 
For example, the same selection as above can be expressed in xpath as follows: 

```{r}
lead = html_node(d, xpath="//div//*[@class='lead']")
html_text(lead)
```

A full explanation of xpath is beyond the scope of this tutorial, 
but you can easily find such an explanation on the internet. 
In general, I would recommend using CSS whenever possible due to the increased readability, and only use xpath if what you're looking for cannot be found using CSS. 

# Scraping with rvest

Now that you have a basic understanding of web scraping and css, let's review how to scrape using RVest step by step.

## Retrieving and parsing a web page

This is the simple part `read_html(url)` will automatically download, read, and parse the web site at url:

```{r}
url = "https://en.wikipedia.org/wiki/World_Happiness_Report"
html_doc = read_html(url) 
```

## Identifting the right node(s)

Using the CSS selectors shown above, you can use `html_node` or `html_nodes` to select nodes from the HTML document. 
Note that the singular form only returns the first result, while the plural form returns all results on the page:

```{r}
html_doc %>% html_node("h2")
html_doc %>% html_nodes("h2")
```

The first call returns only the first `h2` header (the table of contents), while the second call returns all 11 headers. 

## Extracting information from the node

There are four functions that extract information from the selected nodes. First, `html_name` extracts just the tag name of the node, e.g. `p` or `h2`:

```{r}
html_doc %>% html_node("h2") %>% html_name()
```

Next, `html_attr` extracts HTML attributes, e.g. the `href=` of a hyperlink. For example, this selects all hyperlinks from the wikipedia page:

```{r}
urls = html_doc %>% html_nodes("a") %>% html_attr('href') %>%  na.omit()
head(urls)
```

Third, you extract the text contained in an element with `html_text`:

```{r}
html_doc %>% html_node("h1") %>% html_text()
```

Finally, the `html_table` function converts an HTML table into an R data frame. Note, though, that HTML tables are often not as neatly rectangular as a data frame, so unless the table is well formatted the result might not be as nice as you would hope.

```{r}
t= html_doc %>% html_node(".wikitable") %>% html_table()
head(t)
```

## Following links

Often, you will be scraping multiple pages where the first page simply is a list linking to other pages of interest. 
In that case, you will need to extract those links from the first page, and then scrape all the remaining pages.
This can be done using a *for loop* or using a *mapping function*. 

Let's consider the countries in the world hapiness list, and suppose we would like to extract the official name of each country from it's own wiki page (which is linked in the table). 

First, we need to extract the urls from the table.
To make sure we only use the country names, I am specifying that we only want the `a` that is the direct child of the `td` that is the first sibling of the first child of the row, i.e. the second column:

```{r}
urls = html_doc %>% html_node(".wikitable") %>% html_nodes("td:first-child + td > a") %>% html_attr("href")
urls = str_c("https://en.wikipedia.org/", urls)
head(urls)
```

Now, let's extract the name from a single country:

```{r}
read_html(urls[1]) %>% html_node(".country-name") %>% html_text()
```

That worked! But how do we generalize this to all countries?
First, we can use a *for loop*:
(note that to speed things up, we only use the first five results here)

```{r}
results = list()
for (url in urls[1:5]) {
  message(url)
  name = read_html(url) %>% html_node(".country-name") %>% html_text()
  results[[url]] = tibble(name=name)
}
names = bind_rows(results, .id="url")
names
```

This might seem convoluted, but it is relatively idiomatic:
create an empty list, assign the result of each call to an element of the list, and combine the results at the end. 
The benefit of this approach is that is easy to control
and you can e.g. modify it so it skips countries it already scraped
(e.g. by adding `if (url %in% names(results)) continue` just after the for). 
The downside is that it is rather verbose, relatively slow (but for scraping the speed of the R code is rarely the problem compared to how long it takes to retrieve a web page), and somehow un-R-like. 

The more *functional* approach is to `map` the extraction to the elements in the urls vector. 
For this, you define the extraction as a function:

```{r}
get_name = function(url) {
  read_html(url) %>% html_node(".country-name") %>%
    html_text() %>% as_tibble()
}
get_name(urls[1])
```


Now, you can map this function over all urls:

```{r}
map_df(setNames(nm=urls[1:5]), get_name, .id = "url")
```

Note that in order to have the result be a tibble with urls and names (so we can then e.g. join it to the hapiness table), we had to (1) return a tibble rather than just the name, and (2) use the `setNames` function to set the urls as the names on the input.

Finally, you can create an anonymous function using the `~f(.)` notation, which essentially creates `function (x) f(x)`:

```{r}
map_df(setNames(nm=urls[1:5]), 
       ~read_html(.) %>% html_node(".country-name") %>% html_text() %>% as_tibble(), 
       .id = "url") 
```

Whether that is more readable than explicitly defining the function depends on the complexity of the function, but in this case I would certainly argue for the explicit function definition, which is also a lot easier to write and debug. 

(as a challenge, try to extract the population from the infobox rather than the full name. Hint: You might need to use xpath to identify the right cell of the table)