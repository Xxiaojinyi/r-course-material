---
title: 'Web Scraping with RVest'
author: "Wouter van Atteveldt & Kasper Welbers"
date: "2020-10"
output: 
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r, echo=F, message=F}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, message = FALSE, warning = FALSE, fig.keep='none')
library(printr)
```

# What is web scraping and why learn it?

The internet is a veritable data gold mine, and being able to mine this data is a valuable skill set.
In this tutorial we will be looking at a technique called **web scraping**, which can greatly expand your horizon in terms of what data you will be able to collect.

To put this into perspective, let's distinguish three general ways to gather online data.
In the most straightforward situation, you can just **download** some data, for instance as a CSV or JSON file.
This is great if it's possible, but alas, there often is no download button.
Another convenient situation is that some platforms have an **API**.
For example, Twitter has an API where one can collect tweets for a given search term or user.
But what if you encounter data that can't be downloaded, and for which no API is available?
In this case, you might still be able to collect it using **web scraping**.

A simple example is a table on a website.
This table might practically be a data.frame, with nice rows and columns, but it can be hassle to copy this data.
A more elaborate example could be that you want to gather all user posts on a web forum, or all press releases from the website of a certain organization.
You could technically click your way through the website and copy/paste each item manually, but you (or the people you hire) will die a little inside.
Whenever you encounter such a tedious, repetitive data collection task, chances are good you can automate it with a web scraper!

In addition to being a very useful technique, I would furthermore emphasize that web scraping is an excellent way to learn about R and programming.
When students ask how to approach become better in R (or Python, or whatever), our first recommendation is to simply to spend time with it on fun and/or rewarding tasks that intrinsically motivate you.
Web scraping is rather ideal in this regard.
It touches upon various useful programming skills, has an engaging puzzle-like element in trying to conquer websites, and collecting your own novel data set that you could use is pretty awesome.
I mean, It's literally building a robot to do work for you!
How cool is that?

## But wait, is this even allowed?

In principle, yes.
The internet is filled with *robots* that scrape all sorts of content.
But off course, this does not mean that anything goes.
If you write a scraper that opens a webpage a thousand times per minute, you're not being a very nice visitor, and the host might kick you out.
If you collect data from a website, whether manually or automatically, you could run into copyright issues.
For non-commercial research this is rarely an issue, but just remember to always be mindful that not all data is open.

Aside from legal issues, note that there can be ethical concerns as well.
If you scrape data from a web forum where people share deeply personal stories, you can imagine that they might not like this data being collected.
As with any form of research involving people, do consider whether the end goals of your research justify the means.

## Web scraping in a nutshell

In this tutorial we will be using the `rvest` package (to **ha-rvest** data).
This is a neat little package developed by the venerable Hadley Wickham (aka the mastermind behind the `tidyverse`).
And in true tidyverse fashion it makes web scraping really intuitive.
Check out this small piece of code that scrapes the world happiness report from Wikipedia and shows the relationship between wealth and life expectancy.

```{r}
library(rvest)
library(tidyverse)

url = "https://en.wikipedia.org/wiki/World_Happiness_Report"

## import table from the Wikipedia url
happy_table = read_html(url) %>%
  html_element(".wikitable") %>% 
  html_table()

## Plot relationship wealth and life expectancy
ggplot(happy_table, aes(x=`GDP per capita`, y=`Healthy life expectancy`)) + 
  geom_point() + geom_smooth(method = 'lm')
```

The scraping part is really just the short pipe there!
With `read_html(url)`, we visit the Wikipedia page.
Then `html_element(".wikitable")` searches this website to find any elements called 'wikitable', and `html_table()` imports this table as a data frame.

*As an exercise, you could try the same thing for other Wikipedia pages with tables. Just replace the url and change the columns for the x and y axis in gglot*

If you happen to know a bit about HTML, you might realize that `html_element(".wikitable")` just uses CSS to select the (first) element with the .wikitable class.
If so, congratulations, you now basically know web scraping!
If not, don't worry!
You really only need to know very little about HTML to use web scraping.
We'll cover this in the next section.

Off course, this is just a simple example.
If you need to scrape all press releases from a website, you will need more steps and some additional functions from the `rvest` package.
But take a minute to think how this does cover the key logic.
If we would want to scrape all press releases from a website, our first step would be to find an archive on the website that contains links to the press releases.
We would read this archive with `read_html()`, and then use `html_elements()` to look for all HTML elements that contain the links.
Then for each link, we can again use `read_html()` to read the data, and look for all HTML elements of the press release that we want to collect (e.g., title, date, body).





# Web scraping HTML pages in three steps

In this tutorial we focus on web scraping of HTML pages, which covers the vast majority of websites.
The general workflow then covers three steps:

* **Reading HTML pages into R**. This step is by far the easiest. With `rvest` we simply use the `read_html()` function for a given URL. As such, you now already learned this step (one down, two to go!).
* **Selecting HTML elements from these pages**. This step is the most involved, because you need to know a bit about HTML. But even if this is completely new to you, you'll be able to learn the most important steps within a good hour or so.
* **Extracting data from these elements**. This step is again quite easy. `rvest` has some nice, intuitive functions for extracting data from selected HTML elements. 

In this section we'll start with a **short introduction to HTML**, using an example web page that we made for this tutorial.
Then we'll cover **selecting HTML elements** and **extracting data from HTML elements*.


## A short intro to HTML

The vast majority of the internet users **HTML** to make nice looking web pages.
Simply put, HTML is a markup language that tells a browser what things are shown where.
For example, it could say that halfway on the page there is a table, and then tell what data there is in the rows and columns.

In other words, HTML is the language that web developers use to display **data** as a **web page** that's nice for human interpretation.
With web scraping, we're basically translating the **web page** back into **data**.
You really don't need a deep understanding of HTML to do this, but it's convenient to understand the main ideas.

To get a feel for HTML code, open [this link here](https://bit.ly/3lz6ZRe) in your web browser.
Use Chrome or Firefox if you have it (not all browsers let you *inspect elements* as we'll do below).
You should see a nicely formatted document.
Sure, it's not very pretty, but it does have a big bold title, and two ok-ish looking tables.

The purpose of this page is to show an easy example of what the HTML code looks like.
If you right-click on the page, you should see an option like *view page source*.
If you select it you'll see the entire HTML source code.
This can seem a bit overwhelming, but don't worry, you will never actually be reading the entire thing.
We only need to look for the elements that we're interested in, and there are some nice tricks for this that we'll over later.
For now, let's say that we're interested in the table on the left of the page.
Somewhere in the middle of the code you'll find the code for this table (without the `# table` part on the right):

````
<table class="someTable" id="exampleTable">           <!-- table                -->
    <tr class="headerRow">                            <!--    table row         -->
        <th>First column</th>                         <!--       table header   -->
        <th>Second column</th>                        <!--       table header   -->
        <th>Third column</th>                         <!--       table header   -->
    </tr>
    <tr>                                              <!--    table row         -->
        <td>1</td>                                    <!--       table data     -->
        <td>2</td>                                    <!--       table data     -->
        <td>3</td>                                    <!--       table data     -->
    </tr>
    <tr>                                              <!--    table row         -->
        <td>4</td>                                    <!--       table dat      -->
        <td>5</td>                                    <!--       table dat      -->
        <td>6</td>                                    <!--       table data     -->
    </tr>
</table>
````

This is the HTML representation of the table, and it's a good showcase of what HTML is about.
The parts after the `#` are not part of the HTML code, but comments to help you see the structure.
First of all, notice that it has this *family tree* like shape.
At the highest level we have the *table*, that has three *row* children, that in turn also have three children.

This table starts at the opening tag `<table>`, and ends at the closing tag `</table>` (notice the `/`).
Between these tags we see three *table rows* `<tr>`.
Each of these *rows* also has an opening tag `<tr>` and closing tag `</tr>`.
Between these tags we have children representing the values in the cells of each row.
In the first row, these are 3 *table headers* `<th>`, which contain the column names.
The second and third row each have 3 *table data* `<td>`, that contain the data points.

Each of these components can be thought of as an **element** (more strictly a **node**, but the distinction can be ignored for now).
And the cool thing about this hierarchical structure of the HTML code, is that we can look for each of these elements.
This is where the `rvest` package comes in.
With `rvest` we can read the HTML code of a web page into R, look for HTML elements, and extract information from them.  

For example, we see that our table has an id `exampleTable`.
With the `html_element` function, we can use the CCS selector (explained in the next section) to look for this element.

```{r}
url = 'https://bit.ly/3lz6ZRe'

read_html(url) %>%
  html_element('#exampleTable') 
```

The output looks a bit messy, but what it tells us is that we have selected the `<table>` html element/node.
It also shows that this element has the three table rows (tr) as children.
We could now manually extract all data from this element, but this requires us to learn a few more tricks first. 
Note that since this is a table element, you could also use `html_table` like we did above (try it!).

In summary, the vast majority of web pages is written in HTML, and we can extract information from these pages by looking for the HTML elements that we're interested in. 
Accordingly, the main thing to learn about HTML in order to do web scraping is how to look for HTML elements/nodes.


## Selecting HTML elements

The `rvest` package supports two ways for selecting HTML elements. 
The first and default approach is to use **CSS selectors**.
CSS is mostly used by web developers to *style* web pages[^1], but it works just as well for scraping.
The second approach is to use **xpath**. 
Xpath patterns are more difficult to read and write, but are in the end more powerful than CSS selectors, meaning there are some patterns that you can express in xpath but cannot express in CSS.
In this tutorial we'll only cover CCS selectors.
Just be aware that you could also use `xpath`. In `rvest`, you would then simply use `html_element(xpath = "")`. 

[^1]: If you look at the HTML code of our [example page](view-source:https://bit.ly/31keW5P), you see that there is this `<style>...</style>` section, and in this section we also use CSS selectors to select elements. For example, the `.someTable` class is selected to style this table like an APA table, and the `.blue` class defines that any element with this class (in our case the second table) is colored blue.



## CSS selectors

Here we'll cover the main CCS selectors to get you started with web scraping, but a full overview is out of the scope of this tutorial. For some more details you can consult the 
[rvest vignette](https://rvest.tidyverse.org/articles/harvesting-the-web.html), or this [CSS selector overview](https://www.w3schools.com/cssref/css_selectors.asp). 
Alternatively, there is also this nice [game for learning CSS](https://flukeout.github.io/#)

The following CCS selectors cover 

| selector    | example                         | Selects                                     |
|-------------|----------------------------------|---------------------------------------------|
| element/tag | `table`          | **all** `<table>` elements        |
| class       | `.someTable`     | **all** elements with `class="someTable"`  |
| id          | `#steve`  | **unique** element with `id="steve"` |
| element.class | `tr.headerRow`| **all** `<tr>` elements with the `someTable` class |
| class1.class2 | `.someTable.blue`| **all** elements with the `someTable` AND `blue` class |

Let's check them out with our example web page.

```{r, results=F}
html = 'https://bit.ly/3lz6ZRe' %>% read_html

html %>% html_element('table')           ## left table 
html %>% html_element('.someTable')      ## left table
html %>% html_element('#steve')          ## right table 
html %>% html_element('tr.headerRow')    ## left table first row
html %>% html_element('.someTable.blue') ## right table    
```

Note that both the table on the left and on the right have the `someTable` class.
As stated in the table above, we should find **all** elements with this class.
So why does it select only the first one here?
That's because we use `html_element`, which looks for a single element, and
selects the first it finds.

If we want to find all elements we should use the plural `html_elements`.

```{r}
tables = html %>% html_elements('.someTable')
tables
```

Now the output says "xml_nodeset (2)", meaning that the output is not a single element,
but a set containing both of the tables. 
This set behaves like a list, so we can extract each individual table.

```{r}
table2 = tables[[2]]
html_table(table2)
```


## Selecting descendants (children, children's children, etc.)

Sometimes it's necessary to select elements via their parent.
For example, imagine you have a website with news articles, and you want to extract
all URLs mentioned in the news article. You can't just extract all URLs from the web page,
because there are probably many outside of the article as well. 
Instead, what you'd do is select the article element first, and then within that article element
you'd look for all URLs.

Here's how you'd do it. Let's take the [Wikipedia page for hyperlinks](https://en.wikipedia.org/wiki/Hyperlink). And let's say we want to get only
links from the body of the article (so NOT the ones in the left column).
First, let's just get all the links. Links are typically in `<a>` tags, so we'll get all of 
them, and then use the `length()` function to see how many we got.

```{r}
'https://en.wikipedia.org/wiki/Hyperlink' %>%
  read_html() %>%
  html_elements('a') %>%
  length()
```

Now let's do this again, but first selecting only the body. If you inspect the HTML, you'd
find that the body is in an element with the attributes `<div id="content" class="mb-body" role="main">`. 
So we can easily get this with the id selector `#content`. 
There are two ways to do this. The most efficient way is to use `#content a` as the CSS selector.
This basically says: first select `#content`, and then within that select all `<a>`. 

```{r}
'https://en.wikipedia.org/wiki/Hyperlink' %>%
  read_html() %>%
  html_elements('#content a') %>%
  length()
```

Indeed, we got less links this time, because it worked! The nice thing about this is that it works for any combination of CSS selectors. This was a combination of `id` (#content) and  `element` (a), but it could also have been `class element`, `class class` etc.

The second approach is to use the pipe! The `html_elements` function doesn't just work on the entire
HTML. It also works on selected elements. So we could first look for the `#content` element, and then run `html_elements('a')` on that element. 

```{r}
'https://en.wikipedia.org/wiki/Hyperlink' %>%
  read_html() %>%
  html_element('#content') %>%
  html_elements('a') %>%
  length()
```


If this is your first run in with CSS and HTML, this might al seem a bit overwhelming. 
The good part though: this should cover most of what you need!
With just these CSS selectors, and the option to look for elements within elements, you now have a super flexible tool for parsing HTML content. 
We only have one more thing to cover, which is how to extract data from these elements.
Luckily, `rvest` has you covered with some really easy convenience functions.

## `rvest` convenience functions for extracting data from elements

`rvest` offers several nice functions for extracting data from elements. We've already used the
`html_table` and `html_text2` functions, but let's shed a little more light on them.

The `html_table` function doesn't really need much more explanation (right?). Given a table element,
it can produce a data frame in R.

The `html_text` and `html_text2` functions both serve to extract text from an element. They don't 
just get the text directly from the element, but also the text from all its descendants (children, grandchildren, etc.).
The difference between the two is subtle. `html_text` is faster, but gives you the text as it appears in the html code.
For example, look at this text from the left column of our toy example:

```{r}
html = 'https://bit.ly/3lz6ZRe' %>% read_html
html %>% html_element('.leftColumn') %>% html_text()
```

That's pretty ugly. See all those '\n' and empty spaces? That's because in the HTML source code the developer added some line breaks and empty space to make it look
better in the code. But in the browser these extra breaks and spaces are ignored. `html_text2` let's you get the text as seen in the browser. In general, you should just
use html_text2(), but note that for huge amounts of data html_text() might be faster.

```{r}
html %>% html_element('.leftColumn') %>% html_text2()
```

Another nice function is `html_attr` or `html_attrs`, for getting attributes of elements.
With `html_attrs` we get all attributes. For example, we can get the attributes of the `#exampleTable`.

```{r}
html %>% html_elements('#exampleTable') %>% html_attrs()
```

Being able to access attributes is especially useful for scraping links. 
Links are typically represented by `<a>` tags, in which the link is stored as the `href` attribute.

```{r, eval=F}
html %>% html_elements('a') %>% html_attr('href')
```



# Three demo cases

Now that we have the tools to load HTML into R, select HTML elements and extract data from them, we can go over a few cases to demonstrate web scraping in practice.

Please note that since these examples use actual websites, they might break if the websites change. If the code doesn't work anymore, don't judge us too harshly, and please let us know so we can update the examples.


## Scraping a single web page

## Scraping an archive

## Scraping using a sitemap




# Scraping with rvest

Now that you have a basic understanding of web scraping and css, let's review how to scrape using RVest step by step.

## Retrieving and parsing a web page

This is the simple part `read_html(url)` will automatically download, read, and parse the web site at url:

```{r}
url = "https://en.wikipedia.org/wiki/World_Happiness_Report"
html_doc = read_html(url) 
```

## Identifting the right node(s)

Using the CSS selectors shown above, you can use `html_node` or `html_nodes` to select nodes from the HTML document.
Note that the singular form only returns the first result, while the plural form returns all results on the page:

```{r}
html_doc %>% html_node("h2")
html_doc %>% html_nodes("h2")
```

The first call returns only the first `h2` header (the table of contents), while the second call returns all 11 headers.

## Extracting information from the node

There are four functions that extract information from the selected nodes.
First, `html_name` extracts just the tag name of the node, e.g. `p` or `h2`:

```{r}
html_doc %>% html_node("h2") %>% html_name()
```

Next, `html_attr` extracts HTML attributes, e.g. the `href=` of a hyperlink.
For example, this selects all hyperlinks from the wikipedia page:

```{r}
urls = html_doc %>% html_nodes("a") %>% html_attr('href') %>%  na.omit()
head(urls)
```

Third, you extract the text contained in an element with `html_text`:

```{r}
html_doc %>% html_node("h1") %>% html_text()
```

Finally, the `html_table` function converts an HTML table into an R data frame.
Note, though, that HTML tables are often not as neatly rectangular as a data frame, so unless the table is well formatted the result might not be as nice as you would hope.

```{r}
t= html_doc %>% html_node(".wikitable") %>% html_table()
head(t)
```

## Following links

Often, you will be scraping multiple pages where the first page simply is a list linking to other pages of interest.
In that case, you will need to extract those links from the first page, and then scrape all the remaining pages.
This can be done using a *for loop* or using a *mapping function*.

Let's consider the countries in the world hapiness list, and suppose we would like to extract the official name of each country from it's own wiki page (which is linked in the table).

First, we need to extract the urls from the table.
To make sure we only use the country names, I am specifying that we only want the `a` that is the direct child of the `td` that is the first sibling of the first child of the row, i.e. the second column:

```{r}
urls = html_doc %>% html_node(".wikitable") %>% html_nodes("td:first-child + td > a") %>% html_attr("href")
urls = str_c("https://en.wikipedia.org/", urls)
head(urls)
```

Now, let's extract the name from a single country:

```{r}
read_html(urls[1]) %>% html_node(".country-name") %>% html_text()
```

That worked!
But how do we generalize this to all countries?
First, we can use a *for loop*: (note that to speed things up, we only use the first five results here)

```{r}
results = list()
for (url in urls[1:5]) {
  message(url)
  name = read_html(url) %>% html_node(".country-name") %>% html_text()
  results[[url]] = tibble(name=name)
}
names = bind_rows(results, .id="url")
names
```

This might seem convoluted, but it is relatively idiomatic: create an empty list, assign the result of each call to an element of the list, and combine the results at the end.
The benefit of this approach is that is easy to control and you can e.g. modify it so it skips countries it already scraped (e.g. by adding `if (url %in% names(results)) continue` just after the for).
The downside is that it is rather verbose, relatively slow (but for scraping the speed of the R code is rarely the problem compared to how long it takes to retrieve a web page), and somehow un-R-like.

The more *functional* approach is to `map` the extraction to the elements in the urls vector.
For this, you define the extraction as a function:

```{r}
get_name = function(url) {
  read_html(url) %>% html_node(".country-name") %>%
    html_text() %>% as_tibble()
}
get_name(urls[1])
```

Now, you can map this function over all urls:

```{r}
map_df(setNames(nm=urls[1:5]), get_name, .id = "url")
```

Note that in order to have the result be a tibble with urls and names (so we can then e.g. join it to the hapiness table), we had to (1) return a tibble rather than just the name, and (2) use the `setNames` function to set the urls as the names on the input.

Finally, you can create an anonymous function using the `~f(.)` notation, which essentially creates `function (x) f(x)`:

```{r}
map_df(setNames(nm=urls[1:5]), 
       ~read_html(.) %>% html_node(".country-name") %>% html_text() %>% as_tibble(), 
       .id = "url") 
```

Whether that is more readable than explicitly defining the function depends on the complexity of the function, but in this case I would certainly argue for the explicit function definition, which is also a lot easier to write and debug.

(as a challenge, try to extract the population from the infobox rather than the full name. Hint: You might need to use xpath to identify the right cell of the table)
