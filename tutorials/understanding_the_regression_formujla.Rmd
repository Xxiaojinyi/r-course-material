---
title: "Logistic regression: do it yourself"
author: "Kasper Welbers & Wouter van Atteveldt"
date: "January 2020"
output: 
  html_document:
    toc: yes
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F}
knitr::opts_chunk$set(warning=FALSE, results = FALSE, message=FALSE, fig.path = "img/")
```

# This tutorial

This tutorial walks you through the formula of a regression model. It is not intended as a first introduction into regression analysis, but serves to help you develop a better intuition

In particular, we will focus on developing an understanding of the residual term, which is important to understand in order to more on to more complex models, specifically generalized linear models and multilevel models.


# Ordinary linear regression

Regression analysis is a technique used to predict the value of a *dependent* variable from one or multiple *independent* variables. 
Linear regression is used when the dependent variable is continuous, and a linear relation exists between the dependent and (the linear combination of) independent variables. 
A regression model with a single independent variable is then expressed as:

$$y_i = \beta_0 + \beta_1{x}_i + \epsilon_i$$
Here $y$ is the dependent variable and $x$ is the independent variable. 
The $\beta$ parameters represent the intercept $\beta_0$ and slope $\beta_1$ for $x$. 
Given the independent variable and the parameters, we *predict* $y$ as $\beta_0 + \beta_1{x}_i$.
The error of the prediction (i.e. the real $y$ minus the predicted $y$) is represented by the residual term $\epsilon$.
The residuals ($\epsilon_1$, $\epsilon_2$, ..., $\epsilon_n$) are assumed to be independent, normally distributed with mean 0, and constant variance for all values of x. 

## Creating a toy dataset using the regression formula

A good way to develop an intuition for this formula is by using it to create a toy dataset. For this example we set a random seed. This ensures that the results of the random sample are the same for everyone.

```{r}
set.seed(1)
```

First, we'll create $x$ by taking a random sample of 200 *observations* from a normal distribution, for which we can use the `rnorm` function. Here we specify that the mean of this distribution is 4, and the standard deviation is 3. 

```{r}
x = rnorm(n=200, mean=4, sd=3)
```

Now we decide what the intercept (b0) and regression coefficient (b1) should be.

```{r}
b0 = 3
b1 = 0.3
```

For the residuals we draw a random sample of length n from a normal distribution. The mean has to be 0, and the value of the standard deviation will determine how well $x$ can predict $y$ in our data. If standard deviation is zero (and thus the residual is zero for every observation) the prediction would be perfect. 

```{r}
e = rnorm(n=200, mean=0, sd=0.7)
```

Now we can use the regression formula to create $y$.

```{r}
y = b0 + b1*x + e
```

## Analyzing our toy dataset

Now that we have our toy data ($x$ and $y$) we can perform regression analysis.
Here we fit the model, print the model summary, and plot a scatterplot with the regression line.

```{r}
m = lm(y ~ x)
summary(m)
plot(x,y, col='grey')
abline(m, col='green')
```

We see that the coefficients ($\beta0 = 3.00$; $\beta1 = 0.30$) are indeed very close to the *real* values that we used to generate the data ($b0 = 3.05$; $b1 = 0.29$). They are only not identical because we added the random error.

A nice thing about this visualization is that it intuitively distinguishes the structural and random part of the regression equation. The structural part is represented by the line, which is the linear prediction:

$$\beta_0 + \beta_1{x}_i$$
$$3.05 + 0.29x_i$$
The random part is the residual, which is represented by the distance between the line and the dot. You can think of the residual (i.e. that which remains) as the value by which the model fails to correctly predict y. It's also sometimes called the *noise* of the model, or the *error*. 

Let's look at an example. Here we highlight the y value for the 15th observation in our data in blue. The residual is represented as a red line.

```{r}
i = 15
plot(x,y, col='grey')
abline(m, col='green')
points(x[i], y[i], col='blue', pch=10)
segments(x[i], y[i], y1 = m$fitted.values[i], col='red')
```

For this observation, the regression formula is:

$$y_i = \color{green}{\beta_0 + \beta_1{x}_i} + \color{red}{\epsilon_i}$$


$$ 5.90 = \color{green}{3.05 + 0.29*7.37} + \color{red}{0.68}$$
So, we have an observation where $x_i = 7.37$ is 7.37, and $y_i = 5.90$, which is the blue dot in the scatterplot. The prediction for this observation is $3.05 + 0.29*7.37 = 5.22$, which is the point below the blue dot on the green line. The difference of the prediction and the real value of $y_i$ is $5.90 - 5.22 = 0.68$, which is represented by the red line.

Understanding the random component of the regression equation also paves the way to understanding the $R2$ measure. This is a value between 0 and 1 that represent the proportion of variance in $y$ that is explained by the model. 

Recall that the residuals are normally distributed with mean 0. The lower the variance of this distribution, the better the model fit. If the variance is zero (i.e. all dots exactly on the regression line) then the model perfectly explains/predicts $y$ ($R2 = 1$). 

```{r}
y = b0 + b1*x + rnorm(n, mean=0, sd=0)
m = lm(y ~ x)
summary(m)$r.squared
```

Let's break this down a bit. The R2 of 1 tells us that there is no unexplained variance, by which we mean that the predicted value is exactly the real value of y. In a scatterplot, all dots would be exactly on the regression line. 
To make this even more obvious, let's again separate the structural and random/error component

```{r}
structural = b0 + b1*x
error = rnorm(n, mean=0, sd=0)   ## this is just zero
predicted_y = structural         ## so this is just the structural part
```

Now that we have our (perfect) prediction, we can calculate the sum of the squared error (SSE). The error is the difference between the real value of *y*, and the predicted value. So the SSE is literally just squaring these values, and then taking the sum.

```{r}
error = y - predicted_y
squared_error = error^2
sum(squared_error)   
```

So obviously, if there is no error, the sum of the squared error is zero, and our R2 value is 1 (i.e. no unexplained variance). In other words, we know that the lower boundary of the sum of squares is zero. But what then is the upper boundary? 
Let's first add some error to the equation, by setting the standard deviation of the error distribution to 1.

```{r}
structural = b0 + b1*x
error = rnorm(n, mean=0, sd=1)   ## no longer zero
y = structural + error
predicted_y = structural         ## so this is just the structural part
sum((y - predicted_y)^2)
```

So we get a value, but how do we know *how high* this is? It makes sense that 0 is the lowest value, but what is the highest value? How large should the variance be for us to say the the explained variance (R2) is 0?

The answer is again quite intuitive. Our goal is to *explain the variance of y*. So what we want to know is *how much of the variance of y has the model explained*?. Thus, our upper boundary is the variance of y itself. 

```{r}
y - mean(y)
```



## Caveat: the calculation of variance is slightly different



```{r}
structural = b0 + b1*x
error = rnorm(n, mean=0, sd=1)   ## no longer zero
y = structural + error
predicted_y = structural         ## so this is just the structural part

m = lm(y ~ x)
summary(m)

1 - sum((y - predict(m))^2) / sum((y - mean(y))^2)

```



So it makes sense that the least possible variance is no variance.
But how do we know what the largest possible variance is?



that the least amount of unexplained variance is no variance. 

So we know that 

The higher the variance of the residuals, the more variance in $y$ that is unexplained. 

So we 

The total variance in $y$ is calculated as the sum of the squared difference from the mean of $y$. The variance of the residual is calculated as the sum of the squared difference of the 

This means that no variance in y would be unexplained. 


How much variance the model explains is essentially 

```{r}
sum_of_sq <- function(x) sum(x^2)
resid_ss = sum_of_sq(m$residuals)
total_ss = sum_of_sq(y - mean(y))
1 - (resid_ss / total_ss)
```

If the value is 0, then $x$ does not provide any information for predicting $y$. If the value is 1, then there is a deterministic relation between $x$ and $y$. In other words, this would me






# Exercise

As an excersise, please use the code above to create and analyze 